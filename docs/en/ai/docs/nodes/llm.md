# LLM Node (Large Language Model)

## What is it?

The LLM node is used to **invoke AI large language models for reasoning, generation, or analysis tasks**.
You can write prompts here and use variables from the workflow to let the model generate the results you need.

This is the node where the workflow **truly "thinks" and "produces content"**.

## What can you do with it?

#### 1. Generate Text Content

With the LLM node, you can have the model:

* Answer questions
* Write copy or summarize content
* Generate structured text
* Perform role-play style replies

Examples:

* Q&A assistants
* Content generators
* Intelligent customer service replies

---

#### 2. Understand and Analyze Input

You can have the model analyze input data, for example:

* Understand user intent
* Extract key information
* Classify, judge, or score
* Convert input formats

---

#### 3. Build Dynamic Prompts Using Workflow Variables

In the LLM node, you can:

* Reference variables from the Start node or upstream nodes
* Insert variables into Prompts
* Build dynamic, reusable prompts

Example:

```text
User Question: {{query}}
Please provide a clear and concise answer based on the above content.
```

## Core Configuration

#### Prompt

* Use natural language to describe the task you want the model to complete
* Supports referencing workflow variables
* Recommended to clearly describe roles, goals, and output requirements

---

#### Model Parameters (If Supported)

Depending on the platform configuration, you may be able to adjust:

* Model type used
* Output length
* Reply style or stability-related parameters

These parameters directly affect the performance of the generated results.

## Output Results

* The content generated by the LLM serves as the **node output**
* Output results can:

  * Be further processed by subsequent nodes
  * Be returned to the user as the final result
  * Be used for conditional judgment or tool calls

## Usage Rules

* The LLM node must be connected after the Start node or other nodes
* Each execution generates a new result based on the current workflow context
* The node itself does not save state; all data relies on workflow variables

## Usage Recommendations

* Keep Prompts as specific as possible, avoiding vague instructions
* Clearly tell the model your desired **output format**
* For critical workflows, it's recommended to limit output length or structure to improve stability
* Break complex tasks into multiple LLM nodes to increase controllability

## Typical Scenarios

* Dialogue reply generation
* Information summarization and rewriting
* Structured data generation (e.g., JSON, lists)
* Core reasoning node in multi-step workflows
